{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-04ff0c20c54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;31m# linear algebra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Input data files are available in the read-only \"../input/\" directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading input data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/facial-expression/fer2013.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6da9e88aeca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] loading input data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/facial-expression/fer2013.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f.next() for Python 2.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainImages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalImages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalLabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/facial-expression/fer2013.csv'"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading input data...\")\n",
    "f = open(\"/kaggle/input/facial-expression/fer2013.csv\")\n",
    "f.__next__() # f.next() for Python 2.7\n",
    "(trainImages, trainLabels) = ([], [])\n",
    "(valImages, valLabels) = ([], [])\n",
    "(testImages, testLabels) = ([], [])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c87ec62adce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "for row in f:\n",
    "    (label,image,usage) = row.strip().split(\",\")\n",
    "    label=int(label)\n",
    "    image = np.array(image.split(\" \"), dtype=\"uint8\")\n",
    "    image = image.reshape((48, 48))\n",
    "    \n",
    "    if usage == 'Training':\n",
    "        trainImages.append(image)\n",
    "        trainLabels.append(label)\n",
    "    elif usage == 'PrivateTest':\n",
    "        valImages.append(image)\n",
    "        valLabels.append(label)\n",
    "    else:\n",
    "        testImages.append(image)\n",
    "        testLabels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "(trainImages, trainLabels, 'train'),\n",
    "(valImages, valLabels, 'validation'),\n",
    "(testImages, testLabels, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hdf5datasetwriter.py\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "class HDF5DatasetWriter:\n",
    "    def __init__(self, dims, outputPath, dataKey=\"images\",\n",
    "    bufSize=500):\n",
    "        # check to see if the output path exists, and if so, raise\n",
    "        # an exception\n",
    "        if os.path.exists(outputPath):\n",
    "            raise ValueError(\"The supplied ‘outputPath‘ already \"\n",
    "            \"exists and cannot be overwritten.Manually delete \"\n",
    "            \"the file before continuing.\", outputPath)\n",
    "        self.db = h5py.File(outputPath, \"w\")\n",
    "        self.data = self.db.create_dataset(dataKey, dims,\n",
    "                                           dtype=\"float\",compression='gzip',compression_opts=9)\n",
    "        self.labels = self.db.create_dataset(\"labels\", (dims[0],),\n",
    "                                             dtype=\"int\",compression='gzip',compression_opts=9)\n",
    "        self.bufSize = bufSize\n",
    "        self.buffer = {\"data\": [], \"labels\": []}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add(self, rows, labels):\n",
    "\n",
    "        # add the rows and labels to the buffer\n",
    "        self.buffer[\"data\"].extend(rows)\n",
    "        self.buffer[\"labels\"].extend(labels)\n",
    "        if len(self.buffer[\"data\"]) >= self.bufSize:\n",
    "            self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "\n",
    "        # write the buffers to disk then reset the buffer\n",
    "        i = self.idx + len(self.buffer[\"data\"])\n",
    "        self.data[self.idx:i] = self.buffer[\"data\"]\n",
    "        self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
    "        self.idx = i\n",
    "        self.buffer = {\"data\": [], \"labels\": []}\n",
    "\n",
    "    def storeClassLabels(self, classLabels):\n",
    "\n",
    "        # create a dataset to store the actual class label names,\n",
    "        # then store the class labels\n",
    "        dt = h5py.special_dtype(vlen=str)\n",
    "        labelSet = self.db.create_dataset(\"label_names\",\n",
    "                                          (len(classLabels),), dtype=dt)\n",
    "        labelSet[:] = classLabels\n",
    "\n",
    "    def close(self):\n",
    "\n",
    "        # check to see if there are any other entries in the buffer\n",
    "        # that need to be flushed to disk\n",
    "        if len(self.buffer[\"data\"]) > 0:\n",
    "            self.flush()\n",
    "\n",
    "        # close the dataset\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf5datasetwriter import HDF5DatasetWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "for (images,labels,outputPath) in tqdm(datasets):\n",
    "    writer = HDF5DatasetWriter((len(images), 48, 48), outputPath)\n",
    "    for image,label in zip(images,labels):\n",
    "        writer.add([image],[label])\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation,Flatten,Dropout,Dense\n",
    "from keras import backend as K\n",
    "height=48\n",
    "width=48\n",
    "depth=1\n",
    "model=Sequential()\n",
    "inputShape = (height, width, depth)\n",
    "chanDim = -1\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",kernel_initializer=\"he_normal\", input_shape=inputShape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(32, (3, 3), kernel_initializer=\"he_normal\",\n",
    "padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), kernel_initializer=\"he_normal\",\n",
    "padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), kernel_initializer=\"he_normal\",\n",
    "padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), kernel_initializer=\"he_normal\",\n",
    "padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), kernel_initializer=\"he_normal\",\n",
    "padding=\"same\"))\n",
    "model.add(ELU())\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hdf5datasetgenerator.py\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "class HDF5DatasetGenerator:\n",
    "    def __init__(self, dbPath, batchSize, preprocessors=None,\n",
    "    aug=None, binarize=True, classes=2):\n",
    "        # store the batch size, preprocessors, and data augmentor,\n",
    "        # whether or not the labels should be binarized, along with\n",
    "        # the total number of classes\n",
    "        self.batchSize = batchSize\n",
    "        self.preprocessors = preprocessors\n",
    "        self.aug = aug\n",
    "        self.binarize = binarize\n",
    "        self.classes = classes\n",
    "\n",
    "        # open the HDF5 database for reading and determine the total\n",
    "        # number of entries in the database\n",
    "        self.db = h5py.File(dbPath)\n",
    "        self.numImages = self.db[\"labels\"].shape[0]\n",
    "\n",
    "    def generator(self, passes=np.inf):\n",
    "        # initialize the epoch count\n",
    "        epochs = 0\n",
    "        while epochs < passes:\n",
    "            # loop over the HDF5 dataset\n",
    "            for i in np.arange(0, self.numImages, self.batchSize):\n",
    "                # extract the images and labels from the HDF dataset\n",
    "                images = self.db[\"images\"][i: i + self.batchSize]\n",
    "                labels = self.db[\"labels\"][i: i + self.batchSize]\n",
    "                if self.binarize:\n",
    "                    labels = np_utils.to_categorical(labels,\n",
    "                                                     self.classes)\n",
    "\n",
    "                # check to see if our preprocessors are not None\n",
    "                if self.preprocessors is not None:\n",
    "                    # initialize the list of processed images\n",
    "                    procImages = []\n",
    "\n",
    "                    for image in images:\n",
    "                        # loop over the preprocessors and apply each\n",
    "                        # to the image\n",
    "                        for p in self.preprocessors:\n",
    "                            image = p.preprocess(image)\n",
    "                        procImages.append(image)\n",
    "                    images = np.array(procImages)\n",
    "\n",
    "                if self.aug is not None:\n",
    "                    (images, labels) = next(self.aug.flow(images,\n",
    "                                                          labels, batch_size=self.batchSize))\n",
    "                yield (images, labels)\n",
    "\n",
    "            epochs += 1\n",
    "\n",
    "    def close(self):\n",
    "        # close the datab\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile imagetoarraypreprocessor.py\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "class ImageToArrayPreprocessor:\n",
    "    def __init__(self, dataFormat=None):\n",
    "        # store the image data format\n",
    "        self.dataFormat = dataFormat\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        # apply the Keras utility function that correctly rearranges\n",
    "        # the dimensions of the image\n",
    "        return img_to_array(image, data_format=self.dataFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf5datasetgenerator import HDF5DatasetGenerator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from imagetoarraypreprocessor import ImageToArrayPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAug = ImageDataGenerator(rotation_range=10, zoom_range=0.1,\n",
    "horizontal_flip=True, rescale=1 / 255.0, fill_mode=\"nearest\")\n",
    "valAug = ImageDataGenerator(rescale=1 / 255.0)\n",
    "iap = ImageToArrayPreprocessor()\n",
    "\n",
    "trainGen = HDF5DatasetGenerator('/kaggle/working/train', 128,\n",
    "aug=trainAug,preprocessors=[iap],  classes=7)\n",
    "valGen = HDF5DatasetGenerator('/kaggle/working/validation', 128,\n",
    "aug=valAug,preprocessors=[iap], classes=7)\n",
    "\n",
    "\n",
    "opt = Adam(lr=1e-3)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "trainGen.generator(),\n",
    "steps_per_epoch=trainGen.numImages // 128,\n",
    "validation_data=valGen.generator(),\n",
    "validation_steps=valGen.numImages // 128,\n",
    "epochs=50,verbose=1)\n",
    "\n",
    "\n",
    "trainGen.close()\n",
    "valGen.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
